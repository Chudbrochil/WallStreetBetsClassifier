{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SU2OLj_F-JFe"
   },
   "outputs": [],
   "source": [
    "# Set this variable! If you're running in Google colab, set it to true.\n",
    "is_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tLeLYyrEam09"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "if is_colab:\n",
    "    from google.colab import drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x1oIXgV3eNcv"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "if is_colab:\n",
    "    df = pd.read_csv('/content/drive/My Drive/WallStreetBetsClassifier/processed_larger.csv', names=[\"Time\", \"Ticker\", \"Body\", \"Author\", \"Score\", \"Ups\", \"Downs\", \"Controversiality\", \"Gilded\", \"Current_Price\", \"1D\", \"2D\", \"3D\", \"5D\", \"10D\"])\n",
    "else:\n",
    "    df = pd.read_csv('processed_larger.csv', names=[\"Time\", \"Ticker\", \"Body\", \"Author\", \"Score\", \"Ups\", \"Downs\", \"Controversiality\", \"Gilded\", \"Current_Price\", \"1D\", \"2D\", \"3D\", \"5D\", \"10D\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "wSFB9GOPsBLo",
    "outputId": "79c6cfb2-6b64-4ba8-e192-f0219a5db876",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def price_change_trans(pct_change):\n",
    "    if pct_change < -0.05: return 0\n",
    "    elif pct_change < -0.01 and pct_change > -0.05: return 1\n",
    "    elif pct_change < 0.01 and pct_change > -0.01: return 2\n",
    "    elif pct_change < 0.05 and pct_change > 0.01: return 3\n",
    "    else: return 4\n",
    "\n",
    "\n",
    "# Add diffs in price columns, % changed and categorical price changes.\n",
    "def add_price_cols(df):\n",
    "\n",
    "    price_cols = [\"1D\", \"2D\", \"3D\", \"5D\", \"10D\"]\n",
    "\n",
    "    for price in price_cols:\n",
    "        percent_col = pd.DataFrame(df[\"Current_Price\"] / df[price].fillna(0) - 1.00,  columns = [price + \"_pct_change\"])\n",
    "        df = df.join(percent_col)\n",
    "\n",
    "        # Categorical price change\n",
    "        # \"Down-, Down, Neutral, Up, Up+\" = [<-5%, -1% - -5%, -1% - +1%, +1% - +5%, +5%+]\n",
    "        price_change_df = pd.DataFrame(list(df[price + \"_pct_change\"].map(price_change_trans)), columns = [price + \"_cat\"])\n",
    "        df = df.join(price_change_df)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = add_price_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yunEcbPhsU9F"
   },
   "outputs": [],
   "source": [
    "# Make a train, val, test split for our data\n",
    "def train_val_test_split(df, train_frac=0.8, val_frac=0.1):\n",
    "\n",
    "    df.sort_values('Time')\n",
    "    num_rows = len(df)\n",
    "    train = df[0:int(train_frac * num_rows)]\n",
    "    val = df[int(train_frac * num_rows):int(train_frac * num_rows)+int(val_frac * num_rows)]\n",
    "    test = df[int(train_frac * num_rows)+int(val_frac * num_rows):]\n",
    "    return train, val, test\n",
    "\n",
    "train_df, val_df, test_df = train_val_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9PXHTNEdckDV"
   },
   "outputs": [],
   "source": [
    "# Experiment class\n",
    "class Experiment:\n",
    "    def __init__(self, train_df, val_df, model, vectorizer, y_col_name=\"3D\"):\n",
    "        \"\"\"\n",
    "        vectorizer: accepts string and returns numpy array representing embedding for string\n",
    "        Input: \"I think SPY is going up\"\n",
    "        Output: np.array([343, 5674, 345, 3654, 23, 6546, 23434, 4345 234, 345]) \n",
    "        \"\"\"\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.model = model\n",
    "        self.vectorizer = vectorizer\n",
    "        self.X_train = None\n",
    "        self.X_val = None\n",
    "        self.y_train = None \n",
    "        self.y_val = None\n",
    "        self.y_col_name = y_col_name\n",
    "        \n",
    "        self.populate_x_y()\n",
    "\n",
    "    def vectorize_string(self, body):\n",
    "        return self.vectorizer(body)\n",
    "\n",
    "    def populate_x_y(self):\n",
    "        # TODO: Our X could be more than just the body if we want to add more features, e.g. author, score\n",
    "        # Turn into lists for use in sklearn.\n",
    "        self.X_train = list(map(self.vectorizer, list(self.train_df[\"Body\"])))\n",
    "        self.X_val = list(map(self.vectorizer, list(self.val_df[\"Body\"])))\n",
    "        self.y_train = list(self.train_df[self.y_col_name])\n",
    "        self.y_val = list(self.val_df[self.y_col_name])\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        if X is None or y is None:\n",
    "            X = self.X_train\n",
    "            y = self.y_train\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X_test=None):\n",
    "        if X_test is None:\n",
    "            X_test = self.X_val\n",
    "        return self.model.predict(X_test)\n",
    "    \n",
    "    def get_metrics(self, y_pred=None, y_true=None):\n",
    "        \n",
    "        if y_pred is None:\n",
    "            y_pred = self.predict()\n",
    "        if y_true is None:\n",
    "            y_true = self.y_val\n",
    "        \n",
    "        # TODO: Add some more metrics. (F1)\n",
    "        accuracy = metrics.accuracy_score(y_pred, y_true)\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-4rwHrEOc83N",
    "outputId": "a7a42391-1736-4098-d765-0229f99adeee"
   },
   "outputs": [],
   "source": [
    "# Test the Experiment class\n",
    "def dummy_vectorizer(body):\n",
    "    return np.array([54, 54,34, 234 ,345, 345])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rvpnBnrjmllq"
   },
   "outputs": [],
   "source": [
    "# This is from CS224U HW2\n",
    "def glove2dict(src_filename):\n",
    "    \"\"\"GloVe Reader.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    src_filename : str\n",
    "        Full path to the GloVe file to be processed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Mapping words to their GloVe vectors as `np.array`.\n",
    "\n",
    "    \"\"\"\n",
    "    # This distribution has some words with spaces, so we have to\n",
    "    # assume its dimensionality and parse out the lines specially:\n",
    "    if '840B.300d' in src_filename:\n",
    "        line_parser = lambda line: line.rsplit(\" \", 300)\n",
    "    else:\n",
    "        line_parser = lambda line: line.strip().split()\n",
    "    data = {}\n",
    "    with open(src_filename, encoding='utf8') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                line = next(f)\n",
    "                line = line_parser(line)\n",
    "                data[line[0]] = np.array(line[1: ], dtype=np.float)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vd9dvBP6rAhU"
   },
   "outputs": [],
   "source": [
    "# Create a glove vectorizer from 300 dimensional Glove.6B\n",
    "if is_colab:\n",
    "    glove_dict = glove2dict(\"/content/drive/My Drive/WallStreetBetsClassifier/glove.6B.300d.txt\")\n",
    "else:\n",
    "    glove_dict = glove2dict(\"glove.6B.300d.txt\")\n",
    "\n",
    "def glove_vectorizer(body):\n",
    "  unk_vec = np.zeros(300)\n",
    "  result = np.zeros(300)\n",
    "  body_lower = body.lower()\n",
    "  body_split = body_lower.split()\n",
    "  for word in body_split:\n",
    "    vec_for_word = glove_dict.get(word)\n",
    "    if vec_for_word is None:\n",
    "      vec_for_word = unk_vec\n",
    "    result += vec_for_word\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **\"Random\" Guess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22815407355882097\n"
     ]
    }
   ],
   "source": [
    "y_col_name = \"3D_cat\"\n",
    "\n",
    "# Use a normal distribution for our price changes.\n",
    "np.random.seed(224)\n",
    "norm = stats.norm(1.938, 1.223)\n",
    "\n",
    "# Generate numbers according to this normal distribution.\n",
    "y_pred_raw = norm.rvs(size = len(val_df))\n",
    "\n",
    "y_pred = []\n",
    "for val in y_pred_raw:\n",
    "    if val < 0:\n",
    "        y_pred.append(0)\n",
    "    elif val > 4:\n",
    "        y_pred.append(4)\n",
    "    else:\n",
    "        y_pred.append(int(round(val)))\n",
    "\n",
    "# Create an experiment to do a random guess according to a normal distributions.\n",
    "experiment_random = Experiment(train_df, val_df, None, lambda x: x, y_col_name=y_col_name)\n",
    "y_true = experiment_random.y_val\n",
    "\n",
    "print(metrics.accuracy_score(y_pred, y_true))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-350xO23_Zt"
   },
   "source": [
    "### **Linear SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "HJJHFLQptw6Q",
    "outputId": "b74f66a3-88ad-44b5-f672-f68c085f5c6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anthony\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14381358142770193"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Train a model using the 3-day price change category as the predicted value.\n",
    "experiment_svm = Experiment(train_df, val_df, LinearSVC(random_state=0, tol=1e-5, verbose=1, max_iter=1000), glove_vectorizer, y_col_name=y_col_name)\n",
    "experiment_svm.fit()\n",
    "experiment_svm.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aAN8nb-o4xIY"
   },
   "source": [
    "### **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lo4Zismo4c2y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   27.7s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2787583688374924"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get distribution of class weights of training data.\n",
    "class_weights = train_df[y_col_name].value_counts().to_dict()\n",
    "\n",
    "experiment_rf = Experiment(train_df, val_df, RandomForestClassifier(n_estimators = 100, n_jobs=-1, class_weight = class_weights, verbose=1), glove_vectorizer, y_col_name=y_col_name)\n",
    "\n",
    "experiment_rf.fit()\n",
    "experiment_rf.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Classifier (non-GPU Multi-layer Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.87587151\n",
      "Iteration 2, loss = 1.64428090\n",
      "Iteration 3, loss = 1.60641562\n",
      "Iteration 4, loss = 1.57655621\n",
      "Iteration 5, loss = 1.55636955\n",
      "Iteration 6, loss = 1.53479459\n",
      "Iteration 7, loss = 1.53463552\n",
      "Iteration 8, loss = 1.51939784\n",
      "Iteration 9, loss = 1.51363144\n",
      "Iteration 10, loss = 1.50008783\n",
      "Iteration 11, loss = 1.49923838\n",
      "Iteration 12, loss = 1.48529061\n",
      "Iteration 13, loss = 1.47747173\n",
      "Iteration 14, loss = 1.47267526\n",
      "Iteration 15, loss = 1.46614073\n",
      "Iteration 16, loss = 1.46285658\n",
      "Iteration 17, loss = 1.45608438\n",
      "Iteration 18, loss = 1.45170201\n",
      "Iteration 19, loss = 1.45062426\n",
      "Iteration 20, loss = 1.44811444\n",
      "Iteration 21, loss = 1.44433758\n",
      "Iteration 22, loss = 1.43881807\n",
      "Iteration 23, loss = 1.43588942\n",
      "Iteration 24, loss = 1.43115340\n",
      "Iteration 25, loss = 1.42660455\n",
      "Iteration 26, loss = 1.42366940\n",
      "Iteration 27, loss = 1.42194673\n",
      "Iteration 28, loss = 1.42283012\n",
      "Iteration 29, loss = 1.41474092\n",
      "Iteration 30, loss = 1.41028970\n",
      "Iteration 31, loss = 1.40686720\n",
      "Iteration 32, loss = 1.40213290\n",
      "Iteration 33, loss = 1.40117142\n",
      "Iteration 34, loss = 1.39819004\n",
      "Iteration 35, loss = 1.39365289\n",
      "Iteration 36, loss = 1.39029815\n",
      "Iteration 37, loss = 1.38661247\n",
      "Iteration 38, loss = 1.38501181\n",
      "Iteration 39, loss = 1.38560900\n",
      "Iteration 40, loss = 1.38026171\n",
      "Iteration 41, loss = 1.37532744\n",
      "Iteration 42, loss = 1.37276830\n",
      "Iteration 43, loss = 1.37014677\n",
      "Iteration 44, loss = 1.36742948\n",
      "Iteration 45, loss = 1.36491668\n",
      "Iteration 46, loss = 1.36223257\n",
      "Iteration 47, loss = 1.36115136\n",
      "Iteration 48, loss = 1.35948688\n",
      "Iteration 49, loss = 1.35748388\n",
      "Iteration 50, loss = 1.35170029\n",
      "Iteration 51, loss = 1.36179795\n",
      "Iteration 52, loss = 1.35348964\n",
      "Iteration 53, loss = 1.34761143\n",
      "Iteration 54, loss = 1.34400822\n",
      "Iteration 55, loss = 1.34155048\n",
      "Iteration 56, loss = 1.34050623\n",
      "Iteration 57, loss = 1.33980179\n",
      "Iteration 58, loss = 1.33942033\n",
      "Iteration 59, loss = 1.33671441\n",
      "Iteration 60, loss = 1.33516671\n",
      "Iteration 61, loss = 1.33356919\n",
      "Iteration 62, loss = 1.33166402\n",
      "Iteration 63, loss = 1.32985330\n",
      "Iteration 64, loss = 1.33163104\n",
      "Iteration 65, loss = 1.32767167\n",
      "Iteration 66, loss = 1.32535811\n",
      "Iteration 67, loss = 1.32457094\n",
      "Iteration 68, loss = 1.32450328\n",
      "Iteration 69, loss = 1.32079742\n",
      "Iteration 70, loss = 1.31894470\n",
      "Iteration 71, loss = 1.31999435\n",
      "Iteration 72, loss = 1.32816710\n",
      "Iteration 73, loss = 1.32126945\n",
      "Iteration 74, loss = 1.31534166\n",
      "Iteration 75, loss = 1.31326159\n",
      "Iteration 76, loss = 1.31260701\n",
      "Iteration 77, loss = 1.31083385\n",
      "Iteration 78, loss = 1.31014458\n",
      "Iteration 79, loss = 1.30828292\n",
      "Iteration 80, loss = 1.30672175\n",
      "Iteration 81, loss = 1.30615567\n",
      "Iteration 82, loss = 1.30556192\n",
      "Iteration 83, loss = 1.30372905\n",
      "Iteration 84, loss = 1.30325635\n",
      "Iteration 85, loss = 1.30236707\n",
      "Iteration 86, loss = 1.29972501\n",
      "Iteration 87, loss = 1.29981810\n",
      "Iteration 88, loss = 1.29859660\n",
      "Iteration 89, loss = 1.30529711\n",
      "Iteration 90, loss = 1.29657986\n",
      "Iteration 91, loss = 1.29794564\n",
      "Iteration 92, loss = 1.29542895\n",
      "Iteration 93, loss = 1.29467308\n",
      "Iteration 94, loss = 1.29287537\n",
      "Iteration 95, loss = 1.29178372\n",
      "Iteration 96, loss = 1.29136643\n",
      "Iteration 97, loss = 1.29134829\n",
      "Iteration 98, loss = 1.29031182\n",
      "Iteration 99, loss = 1.29340439\n",
      "Iteration 100, loss = 1.28835588\n",
      "Iteration 101, loss = 1.28709856\n",
      "Iteration 102, loss = 1.28995598\n",
      "Iteration 103, loss = 1.28682945\n",
      "Iteration 104, loss = 1.28556796\n",
      "Iteration 105, loss = 1.28437278\n",
      "Iteration 106, loss = 1.28257031\n",
      "Iteration 107, loss = 1.28633590\n",
      "Iteration 108, loss = 1.28174056\n",
      "Iteration 109, loss = 1.27888041\n",
      "Iteration 110, loss = 1.28021459\n",
      "Iteration 111, loss = 1.27920292\n",
      "Iteration 112, loss = 1.27970765\n",
      "Iteration 113, loss = 1.27830684\n",
      "Iteration 114, loss = 1.27811027\n",
      "Iteration 115, loss = 1.27775992\n",
      "Iteration 116, loss = 1.27639367\n",
      "Iteration 117, loss = 1.27545778\n",
      "Iteration 118, loss = 1.27433123\n",
      "Iteration 119, loss = 1.27458500\n",
      "Iteration 120, loss = 1.27480758\n",
      "Iteration 121, loss = 1.27643165\n",
      "Iteration 122, loss = 1.27283819\n",
      "Iteration 123, loss = 1.27146087\n",
      "Iteration 124, loss = 1.27064812\n",
      "Iteration 125, loss = 1.27020421\n",
      "Iteration 126, loss = 1.26824808\n",
      "Iteration 127, loss = 1.26687385\n",
      "Iteration 128, loss = 1.26800904\n",
      "Iteration 129, loss = 1.26951654\n",
      "Iteration 130, loss = 1.26728818\n",
      "Iteration 131, loss = 1.27347638\n",
      "Iteration 132, loss = 1.26864667\n",
      "Iteration 133, loss = 1.26500001\n",
      "Iteration 134, loss = 1.26378410\n",
      "Iteration 135, loss = 1.26477795\n",
      "Iteration 136, loss = 1.26344320\n",
      "Iteration 137, loss = 1.26319815\n",
      "Iteration 138, loss = 1.26276943\n",
      "Iteration 139, loss = 1.26357126\n",
      "Iteration 140, loss = 1.26095655\n",
      "Iteration 141, loss = 1.25976844\n",
      "Iteration 142, loss = 1.25981324\n",
      "Iteration 143, loss = 1.26144790\n",
      "Iteration 144, loss = 1.25989119\n",
      "Iteration 145, loss = 1.25873629\n",
      "Iteration 146, loss = 1.26720128\n",
      "Iteration 147, loss = 1.25776500\n",
      "Iteration 148, loss = 1.25544892\n",
      "Iteration 149, loss = 1.25717799\n",
      "Iteration 150, loss = 1.25917558\n",
      "Iteration 151, loss = 1.25575550\n",
      "Iteration 152, loss = 1.25584103\n",
      "Iteration 153, loss = 1.25525346\n",
      "Iteration 154, loss = 1.25458850\n",
      "Iteration 155, loss = 1.25271036\n",
      "Iteration 156, loss = 1.25349127\n",
      "Iteration 157, loss = 1.25303315\n",
      "Iteration 158, loss = 1.25253451\n",
      "Iteration 159, loss = 1.25017530\n",
      "Iteration 160, loss = 1.25292479\n",
      "Iteration 161, loss = 1.25287583\n",
      "Iteration 162, loss = 1.25008117\n",
      "Iteration 163, loss = 1.25017149\n",
      "Iteration 164, loss = 1.25009955\n",
      "Iteration 165, loss = 1.25065190\n",
      "Iteration 166, loss = 1.24707183\n",
      "Iteration 167, loss = 1.24725491\n",
      "Iteration 168, loss = 1.24833603\n",
      "Iteration 169, loss = 1.25405948\n",
      "Iteration 170, loss = 1.25500500\n",
      "Iteration 171, loss = 1.25341694\n",
      "Iteration 172, loss = 1.24925314\n",
      "Iteration 173, loss = 1.24486451\n",
      "Iteration 174, loss = 1.24368779\n",
      "Iteration 175, loss = 1.24723057\n",
      "Iteration 176, loss = 1.24373998\n",
      "Iteration 177, loss = 1.24403189\n",
      "Iteration 178, loss = 1.24488820\n",
      "Iteration 179, loss = 1.24187865\n",
      "Iteration 180, loss = 1.24163556\n",
      "Iteration 181, loss = 1.24457162\n",
      "Iteration 182, loss = 1.24146752\n",
      "Iteration 183, loss = 1.24047640\n",
      "Iteration 184, loss = 1.24031307\n",
      "Iteration 185, loss = 1.24353397\n",
      "Iteration 186, loss = 1.24053001\n",
      "Iteration 187, loss = 1.24036536\n",
      "Iteration 188, loss = 1.23938084\n",
      "Iteration 189, loss = 1.23759537\n",
      "Iteration 190, loss = 1.24059391\n",
      "Iteration 191, loss = 1.24066627\n",
      "Iteration 192, loss = 1.23788065\n",
      "Iteration 193, loss = 1.23913364\n",
      "Iteration 194, loss = 1.23711023\n",
      "Iteration 195, loss = 1.23863365\n",
      "Iteration 196, loss = 1.23887984\n",
      "Iteration 197, loss = 1.23469918\n",
      "Iteration 198, loss = 1.23720976\n",
      "Iteration 199, loss = 1.23742032\n",
      "Iteration 200, loss = 1.23809430\n",
      "Iteration 201, loss = 1.23456992\n",
      "Iteration 202, loss = 1.23566413\n",
      "Iteration 203, loss = 1.23472896\n",
      "Iteration 204, loss = 1.23390485\n",
      "Iteration 205, loss = 1.23423223\n",
      "Iteration 206, loss = 1.23340154\n",
      "Iteration 207, loss = 1.23848209\n",
      "Iteration 208, loss = 1.23783957\n",
      "Iteration 209, loss = 1.23307640\n",
      "Iteration 210, loss = 1.23161995\n",
      "Iteration 211, loss = 1.23219015\n",
      "Iteration 212, loss = 1.23424291\n",
      "Iteration 213, loss = 1.23136103\n",
      "Iteration 214, loss = 1.23086220\n",
      "Iteration 215, loss = 1.23272152\n",
      "Iteration 216, loss = 1.22941973\n",
      "Iteration 217, loss = 1.23031468\n",
      "Iteration 218, loss = 1.22887635\n",
      "Iteration 219, loss = 1.22968019\n",
      "Iteration 220, loss = 1.23374049\n",
      "Iteration 221, loss = 1.23934549\n",
      "Iteration 222, loss = 1.22788504\n",
      "Iteration 223, loss = 1.22705712\n",
      "Iteration 224, loss = 1.22648873\n",
      "Iteration 225, loss = 1.22573044\n",
      "Iteration 226, loss = 1.22646847\n",
      "Iteration 227, loss = 1.22661327\n",
      "Iteration 228, loss = 1.22796269\n",
      "Iteration 229, loss = 1.22604534\n",
      "Iteration 230, loss = 1.22609534\n",
      "Iteration 231, loss = 1.22622738\n",
      "Iteration 232, loss = 1.22597387\n",
      "Iteration 233, loss = 1.23638951\n",
      "Iteration 234, loss = 1.22720493\n",
      "Iteration 235, loss = 1.22533371\n",
      "Iteration 236, loss = 1.22529143\n",
      "Iteration 237, loss = 1.22646798\n",
      "Iteration 238, loss = 1.22379398\n",
      "Iteration 239, loss = 1.22436005\n",
      "Iteration 240, loss = 1.22305345\n",
      "Iteration 241, loss = 1.22424844\n",
      "Iteration 242, loss = 1.22385408\n",
      "Iteration 243, loss = 1.22265245\n",
      "Iteration 244, loss = 1.22178167\n",
      "Iteration 245, loss = 1.22222063\n",
      "Iteration 246, loss = 1.22324276\n",
      "Iteration 247, loss = 1.22122787\n",
      "Iteration 248, loss = 1.22053989\n",
      "Iteration 249, loss = 1.22105145\n",
      "Iteration 250, loss = 1.22092240\n",
      "Iteration 251, loss = 1.22306841\n",
      "Iteration 252, loss = 1.21910943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 1.22090194\n",
      "Iteration 254, loss = 1.22097812\n",
      "Iteration 255, loss = 1.22346034\n",
      "Iteration 256, loss = 1.21897671\n",
      "Iteration 257, loss = 1.21925327\n",
      "Iteration 258, loss = 1.22043205\n",
      "Iteration 259, loss = 1.21868496\n",
      "Iteration 260, loss = 1.21781023\n",
      "Iteration 261, loss = 1.21970039\n",
      "Iteration 262, loss = 1.21729991\n",
      "Iteration 263, loss = 1.21747387\n",
      "Iteration 264, loss = 1.21827403\n",
      "Iteration 265, loss = 1.21710210\n",
      "Iteration 266, loss = 1.21694687\n",
      "Iteration 267, loss = 1.21514507\n",
      "Iteration 268, loss = 1.22759805\n",
      "Iteration 269, loss = 1.22000239\n",
      "Iteration 270, loss = 1.21494876\n",
      "Iteration 271, loss = 1.21603542\n",
      "Iteration 272, loss = 1.21319518\n",
      "Iteration 273, loss = 1.21851002\n",
      "Iteration 274, loss = 1.21509107\n",
      "Iteration 275, loss = 1.21400268\n",
      "Iteration 276, loss = 1.21456361\n",
      "Iteration 277, loss = 1.21435373\n",
      "Iteration 278, loss = 1.21336915\n",
      "Iteration 279, loss = 1.21313315\n",
      "Iteration 280, loss = 1.21328006\n",
      "Iteration 281, loss = 1.21461312\n",
      "Iteration 282, loss = 1.21415604\n",
      "Iteration 283, loss = 1.21427192\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26867228936614207"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(activation='relu', solver='adam', max_iter=500, verbose=True)\n",
    "experiment_mlp = Experiment(train_df, val_df, mlp, glove_vectorizer, y_col_name=y_col_name)\n",
    "experiment_mlp.fit()\n",
    "experiment_mlp.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IW5B9wXl9Oqb"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 25524, 2: 23295, 3: 20305, 4: 11589, 0: 11299}\n",
      "0.2773986001825849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anthony\\anaconda3\\lib\\site-packages\\numpy\\lib\\histograms.py:839: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  keep = (tmp_a >= first_edge)\n",
      "C:\\Users\\anthony\\anaconda3\\lib\\site-packages\\numpy\\lib\\histograms.py:840: RuntimeWarning: invalid value encountered in less_equal\n",
      "  keep &= (tmp_a <= last_edge)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Distribution of the training data according to categorical price changes.\n",
    "class_weights = train_df[y_col_name].value_counts().to_dict()\n",
    "print(class_weights)\n",
    "print(class_weights[1] / sum(class_weights.values()))\n",
    "\n",
    "\n",
    "\n",
    "# Get occurrences, but we want whole number percents. Multiply by 100\n",
    "occurrences = list(df['3D_pct_change'].apply(lambda x: x*100))\n",
    "\n",
    "n, bins, patches = plt.hist(occurrences, 1000)\n",
    "plt.xlim(-10, 10)\n",
    "plt.xlabel(\"Percent change over 3 days\")\n",
    "plt.ylabel(\"Occurrences\")\n",
    "plt.title(\"Continuous % change for 14 stocks\")\n",
    "#plt.show()\n",
    "plt.savefig(\"continuous.png\")\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "# Graph for discrete price change categories\n",
    "occurrences_dict = {\"Down-\" : (df.loc[df['3D_pct_change'] < -0.05]).shape[0], \n",
    "            \"Down\" : (df.loc[(df['3D_pct_change'] >= -0.05) & (df['3D_pct_change'] <= -0.01)]).shape[0],\n",
    "            \"Neutral\" : (df.loc[(df['3D_pct_change'] >= -0.01) & (df['3D_pct_change'] <= 0.01)]).shape[0],\n",
    "            \"Up\" : (df.loc[(df['3D_pct_change'] >= 0.01) & (df['3D_pct_change'] <= 0.05)]).shape[0],\n",
    "            \"Up+\" : (df.loc[df['3D_pct_change'] >= 0.05]).shape[0]}\n",
    "\n",
    "plt.bar(occurrences_dict.keys(), occurrences_dict.values())\n",
    "plt.xlabel(\"Classification\")\n",
    "plt.ylabel(\"Occurrences\")\n",
    "plt.title(\"Discrete % change for 14 stocks\")\n",
    "#plt.show()\n",
    "plt.savefig(\"discrete.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sample_model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
